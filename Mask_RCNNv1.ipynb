{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Mask_RCNNv1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "nU9abGk3brV8",
        "SV9QaKCztXJC"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/getcontrol/maskrcnn-hacks/blob/main/Mask_RCNNv1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jt8zBlsKzcT"
      },
      "source": [
        "#**Mask RCNN Training on Skymaps**\n",
        "\n",
        "Credits: Nancy Cai\n",
        "\n",
        "##**Reference:**\n",
        "1. [Github-Mask RCNN Model](https://github.com/matterport/Mask_RCNN)\n",
        "2. [Article-Splash of Color: Instance Segmentation with Mask R-CNN and TensorFlow](https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46)\n",
        "3. [Github-Mask R-CNN Training and Inference](https://github.com/akTwelve/cocosynth/blob/master/notebooks/train_mask_rcnn.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeU09AQ1x86H"
      },
      "source": [
        "##**Set Up**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exOpWVe4NSBM"
      },
      "source": [
        "%cd /content/\n",
        "!rm -r sample_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-1aSzxeyEja"
      },
      "source": [
        "from google.colab import drive    # RUN WHEN TRAINED WEIGHTS GET UPLOADED TO THE GOOGLE DRIVE,  \n",
        "drive.mount('/content/drive')     # so you can access files in google drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-3k5AC4ic_H"
      },
      "source": [
        "##**GPU**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCD-FT5Zxqy-"
      },
      "source": [
        "# GPU for training.\n",
        "DEVICE = \"/gpu:0\"  # /cpu:0 or /gpu:0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6NRSCRWKPpU"
      },
      "source": [
        "##**Setting Up Folder Structure**\n",
        "\n",
        "1.   Mask_RCNN\n",
        "2.   data\n",
        "3.   weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX0fYtQdP2yA"
      },
      "source": [
        "%%capture\n",
        "!rm -rf sample_data\n",
        "!mkdir data\n",
        "!mkdir weights\n",
        "!gdown https://drive.google.com/u/0/uc?id=1eToBctRfz2b83KgP25xRsJ2mZAeqYtHB -O ./data/data.zip\n",
        "!unzip ./data/data.zip -d ./data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO6SZziDHTzL"
      },
      "source": [
        "!wget -P weights/ https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzXEWTnySBju"
      },
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/matterport/Mask_RCNN "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXQv0SpezA-F"
      },
      "source": [
        " %cd /content/Mask_RCNN\n",
        " !python setup.py install\n",
        " %cd /content/\n",
        " !ls #make sure root conrent listed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYzJ_l0pcHg9"
      },
      "source": [
        "!pip install tensorflow==1.15.0 \n",
        "!pip uninstall keras-nightly -y\n",
        "!pip install h5py==2.10.0 # resolve the issue AttributeError: 'str' object has no attribute 'decode' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAnmbP7ddXQQ"
      },
      "source": [
        "!pip install keras==2.1.0 --force-reinstall --no-deps --no-cache-dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwQiWGh6dYdb"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKrX6x9SKZ-8"
      },
      "source": [
        "##**Import libs & Model dependent libs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynf0uEN3P6D1"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import datetime\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "# plot images\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# process images\n",
        "from PIL import Image\n",
        "from collections import defaultdict\n",
        "\n",
        "# load data\n",
        "from pathlib import Path\n",
        "import json\n",
        "import glob\n",
        "\n",
        "# process coco data\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from pycocotools import mask as maskUtils\n",
        "\n",
        "# Evaluate\n",
        "import cv2\n",
        "import skimage\n",
        "from skimage.metrics import structural_similarity as ssim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "El2mQWlK8Qzs"
      },
      "source": [
        "# Change directory into cloned repo\n",
        "%cd /content/Mask_RCNN\n",
        "\n",
        "# List repo contents\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViisiB1A_ei6"
      },
      "source": [
        "from mrcnn import utils\n",
        "from mrcnn import visualize\n",
        "from mrcnn.visualize import display_images\n",
        "from mrcnn.visualize import display_instances\n",
        "import mrcnn.model as modellib\n",
        "from mrcnn.model import log\n",
        "from mrcnn.config import Config\n",
        "from mrcnn import model as modellib, utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnNiqFaBKk9a"
      },
      "source": [
        "##**Split Train and Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYpo2hhbQD_5"
      },
      "source": [
        "dataroot = Path(\"/content/data\")\n",
        "\n",
        "def get_filenames(directory):\n",
        "  filenames = [file for file in directory.rglob(\"*\") if file.is_file()]\n",
        "  return filenames\n",
        "\n",
        "inputs_train=get_filenames(dataroot/\"imgs\"/\"train\")\n",
        "targets_train=get_filenames(dataroot/\"masks\"/\"train\")\n",
        "\n",
        "inputs_valid=get_filenames(dataroot/\"imgs\"/\"validation\")\n",
        "targets_valid=get_filenames(dataroot/\"masks\"/\"validation\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDQn2o8nMhK3"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLWjev9JmeL5"
      },
      "source": [
        "**Plot Train & Masked Images**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX7PBOWESz5R"
      },
      "source": [
        "# code for displaying multiple images \n",
        "# create figure\n",
        "import os\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "  \n",
        "def plot_images(rows, columns):\n",
        "  # reading images\n",
        "  each_images = int(rows*columns/2)\n",
        "  for i in range(1,each_images+1):\n",
        "      # Adds subplots for train images\n",
        "      input_full_path = inputs_train[i].as_uri()\n",
        "      relative_path = 'file:'\n",
        "      input_image = '/' + os.path.relpath(input_full_path, relative_path)\n",
        "      trainImage = mpimg.imread(input_image)\n",
        "      trainImageName = inputs_train[i].name\n",
        "      fig.add_subplot(rows, columns, i)\n",
        "      plt.imshow(trainImage)\n",
        "      plt.axis('off')\n",
        "      plt.title(f\"train_{i}\")\n",
        "\n",
        "      # Adds suplots for target images\n",
        "      target_full_path = targets_train[i].as_uri()\n",
        "      target_image = '/' + os.path.relpath(target_full_path, relative_path)\n",
        "      targetImage = mpimg.imread(target_image)\n",
        "      targetImageName = targets_train[i].name\n",
        "      fig.add_subplot(rows, columns, columns+i)\n",
        "      plt.imshow(targetImage)\n",
        "      plt.axis('off')\n",
        "      plt.title(f\"target_{i}\")\n",
        "\n",
        "plot_images(2,5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jdtif0QopyvV"
      },
      "source": [
        "# Get colours used in the images to define our annotation function\n",
        "%cd /content/data/masks/train/\n",
        "\n",
        "im = Image.open(targets_train[4].name)\n",
        "by_color = defaultdict(int)\n",
        "for pixel in im.getdata(): \n",
        "  by_color[pixel] += 1\n",
        "by_color\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvp8WTybdX4z"
      },
      "source": [
        "##**Create annotation files based on masked data**\n",
        "You could run this once and download the annotations for model training because it took more than one hour to get annotation files. I have saved the annotations files to [Google Driver](https://drive.google.com/drive/folders/1_WUjJ3ArrJLSvzvLcTDeSVQhcg-Gr3IV)\n",
        "\n",
        "Mask RCNN needs annotation files for training and because this dataset is public and do not have annotations files, we generated annotaions using python script. \n",
        "\n",
        "**Reference**\n",
        "1. [Article-Create COCO Annotations from Scratch](https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch) \n",
        "2. [Github-Image to COCO JSON Converter](https://github.com/chrise96/image-to-coco-json-converter) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9SDEZ0n75qp"
      },
      "source": [
        "%cd /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4PuG48Ddes5"
      },
      "source": [
        "!git clone -l -s https://github.com/chrise96/image-to-coco-json-converter\n",
        "\n",
        "# Change directory into cloned repo\n",
        "%cd image-to-coco-json-converter\n",
        "\n",
        "# List repo contents\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVu6r9UddXIB"
      },
      "source": [
        "# Label ids of the dataset\n",
        "category_ids = {\n",
        "    \"background\": 0,\n",
        "    \"weed\": 1\n",
        "#    \"plant\": 2 comment out plant \n",
        "}\n",
        "\n",
        "# Define which colors match which categories in the images\n",
        "category_colors = {\n",
        "    \"(0, 0, 0)\": 0, # background\n",
        "    \"(2, 2, 2)\": 0,\n",
        "    \"(39, 39, 39)\": 1, # weed\n",
        "    \"(78, 78, 78)\": 0 # change plant to background because only want to detect weed\n",
        "}\n",
        "\n",
        "# Define the ids that are a multiplolygon. In our case: weed\n",
        "multipolygon_ids = [1] #[1,2]\n",
        "\n",
        "# Get \"images\" and \"annotations\" info \n",
        "def images_annotations_info(maskpath):\n",
        "    # This id will be automatically increased as we go\n",
        "    annotation_id = 0\n",
        "    image_id = 0\n",
        "    annotations = []\n",
        "    images = []\n",
        "    \n",
        "    for mask_image in glob.glob(maskpath + \"*.png\"):\n",
        "        # The mask image is *.png but the original image is *.jpg.\n",
        "        # We make a reference to the original file in the COCO JSON file\n",
        "        original_file_name = os.path.basename(mask_image)\n",
        "\n",
        "        # Open the image and (to be sure) we convert it to RGB\n",
        "        mask_image_open = Image.open(mask_image).convert(\"RGB\")\n",
        "        w, h = mask_image_open.size\n",
        "        \n",
        "        # \"images\" info \n",
        "        image = create_image_annotation(original_file_name, w, h, image_id)\n",
        "        images.append(image)\n",
        "\n",
        "        sub_masks = create_sub_masks(mask_image_open, w, h)\n",
        "        for color, sub_mask in sub_masks.items():\n",
        "            category_id = category_colors[color]\n",
        "\n",
        "            # \"annotations\" info\n",
        "            polygons, segmentations = create_sub_mask_annotation(sub_mask)\n",
        "\n",
        "            # Check if we have classes that are a multipolygon\n",
        "            if category_id in multipolygon_ids:\n",
        "                # Combine the polygons to calculate the bounding box and area\n",
        "                multi_poly = MultiPolygon(polygons)\n",
        "                                \n",
        "                annotation = create_annotation_format(multi_poly, segmentations, image_id, category_id, annotation_id)\n",
        "\n",
        "                annotations.append(annotation)\n",
        "                annotation_id += 1\n",
        "            else:\n",
        "                for i in range(len(polygons)):\n",
        "                    # Cleaner to recalculate this variable\n",
        "                    segmentation = [np.array(polygons[i].exterior.coords).ravel().tolist()]\n",
        "                    \n",
        "                    annotation = create_annotation_format(polygons[i], segmentation, image_id, category_id, annotation_id)\n",
        "                    \n",
        "                    annotations.append(annotation)\n",
        "                    annotation_id += 1\n",
        "        image_id += 1\n",
        "    return images, annotations, annotation_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RILGfhY7HSe5"
      },
      "source": [
        "!touch /content/data/imgs/train/train.json\n",
        "!touch /content/data/imgs/validation/validation.json\n",
        "!touch /content/data/imgs/test/test.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6On-Z3xa8Xu_"
      },
      "source": [
        "from src.create_annotations import *\n",
        "\n",
        "coco_format = get_coco_json_format()\n",
        "for keyword in [\"train\",\"validation\",\"test\"]:\n",
        "  mask_path = \"/content/data/masks/{}/\".format(keyword)\n",
        "  annotation_path = \"/content/data/imgs/{}/\".format(keyword)\n",
        "  print(mask_path, annotation_path)\n",
        "        \n",
        "  # Create category section\n",
        "  coco_format[\"categories\"] = create_category_annotation(category_ids)\n",
        "    \n",
        "  # Create images and annotations sections\n",
        "  coco_format[\"images\"], coco_format[\"annotations\"], annotation_cnt = images_annotations_info(mask_path)\n",
        "\n",
        "  with open(\"{}/{}.json\".format(annotation_path,keyword),\"w\") as outfile:\n",
        "    json.dump(coco_format, outfile)\n",
        "        \n",
        "  print(\"Created %d annotations for images in folder: %s\" % (annotation_cnt, annotation_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um6_7gONBkUU"
      },
      "source": [
        "##**View Annotations**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYhY9wZBBkUZ"
      },
      "source": [
        "import IPython\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import base64\n",
        "from math import trunc\n",
        "from PIL import Image as PILImage\n",
        "from PIL import ImageDraw as PILImageDraw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4gyVE4JBkUb"
      },
      "source": [
        "# Load the dataset json\n",
        "class CocoDataset():\n",
        "    def __init__(self, annotation_path, image_dir):\n",
        "        self.annotation_path = annotation_path\n",
        "        self.image_dir = image_dir\n",
        "        self.colors = colors = ['blue', 'purple', 'red', 'green', 'orange', 'salmon', 'pink', 'gold',\n",
        "                                'orchid', 'slateblue', 'limegreen', 'seagreen', 'darkgreen', 'olive',\n",
        "                               'teal', 'aquamarine', 'steelblue', 'powderblue', 'dodgerblue', 'navy',\n",
        "                               'magenta', 'sienna', 'maroon']\n",
        "        \n",
        "        json_file = open(self.annotation_path)\n",
        "        self.coco = json.load(json_file)\n",
        "        json_file.close()\n",
        "        \n",
        "        self.process_info()\n",
        "        self.process_licenses()\n",
        "        self.process_categories()\n",
        "        self.process_images()\n",
        "        self.process_segmentations()\n",
        "            \n",
        "        \n",
        "    def display_info(self):\n",
        "        print('Dataset Info:')\n",
        "        print('=============')\n",
        "        for key, item in self.info.items():\n",
        "            print('  {}: {}'.format(key, item))\n",
        "        \n",
        "        requirements = [['description', str],\n",
        "                        ['url', str],\n",
        "                        ['version', str],\n",
        "                        ['year', int],\n",
        "                        ['contributor', str],\n",
        "                        ['date_created', str]]\n",
        "        for req, req_type in requirements:\n",
        "            if req not in self.info:\n",
        "                print('ERROR: {} is missing'.format(req))\n",
        "            elif type(self.info[req]) != req_type:\n",
        "                print('ERROR: {} should be type {}'.format(req, str(req_type)))\n",
        "        print('')\n",
        "\n",
        "        \n",
        "    def display_licenses(self):\n",
        "        print('Licenses:')\n",
        "        print('=========')\n",
        "        \n",
        "        requirements = [['id', int],\n",
        "                        ['url', str],\n",
        "                        ['name', str]]\n",
        "        for license in self.licenses:\n",
        "            for key, item in license.items():\n",
        "                print('  {}: {}'.format(key, item))\n",
        "            for req, req_type in requirements:\n",
        "                if req not in license:\n",
        "                    print('ERROR: {} is missing'.format(req))\n",
        "                elif type(license[req]) != req_type:\n",
        "                    print('ERROR: {} should be type {}'.format(req, str(req_type)))\n",
        "            print('')\n",
        "        print('')\n",
        "        \n",
        "    def display_categories(self):\n",
        "        print('Categories:')\n",
        "        print('=========')\n",
        "        for sc_key, sc_val in self.super_categories.items():\n",
        "            print('  super_category: {}'.format(sc_key))\n",
        "            for cat_id in sc_val:\n",
        "                print('    id {}: {}'.format(cat_id, self.categories[cat_id]['name']))\n",
        "            print('')\n",
        "    \n",
        "    def display_image(self, image_id, show_polys=True, show_bbox=True, show_crowds=True, use_url=False):\n",
        "        print('Image:')\n",
        "        print('======')\n",
        "        if image_id == 'random':\n",
        "            image_id = random.choice(list(self.images.keys()))\n",
        "        \n",
        "        # Print the image info\n",
        "        image = self.images[image_id]\n",
        "        for key, val in image.items():\n",
        "            print('  {}: {}'.format(key, val))\n",
        "            \n",
        "        # Open the image\n",
        "        if use_url:\n",
        "            image_path = image['coco_url']\n",
        "            response = requests.get(image_path)\n",
        "            image = PILImage.open(BytesIO(response.content))\n",
        "            \n",
        "        else:\n",
        "            image_path = os.path.join(self.image_dir, image['file_name'])\n",
        "            image = PILImage.open(image_path)\n",
        "            \n",
        "            buffer = BytesIO()\n",
        "            image.save(buffer, format='PNG')\n",
        "            buffer.seek(0)\n",
        "            \n",
        "            data_uri = base64.b64encode(buffer.read()).decode('ascii')\n",
        "            image_path = \"data:image/png;base64,{0}\".format(data_uri)\n",
        "            \n",
        "        # Calculate the size and adjusted display size\n",
        "        max_width = 600\n",
        "        image_width, image_height = image.size\n",
        "        adjusted_width = min(image_width, max_width)\n",
        "        adjusted_ratio = adjusted_width / image_width\n",
        "        adjusted_height = adjusted_ratio * image_height\n",
        "        \n",
        "        # Create list of polygons to be drawn\n",
        "        polygons = {}\n",
        "        bbox_polygons = {}\n",
        "        rle_regions = {}\n",
        "        poly_colors = {}\n",
        "        print('  segmentations ({}):'.format(len(self.segmentations[image_id])))\n",
        "        for i, segm in enumerate(self.segmentations[image_id]):\n",
        "            polygons_list = []\n",
        "            if segm['iscrowd'] != 0:\n",
        "                # Gotta decode the RLE\n",
        "                px = 0\n",
        "                x, y = 0, 0\n",
        "                rle_list = []\n",
        "                for j, counts in enumerate(segm['segmentation']['counts']):\n",
        "                    if j % 2 == 0:\n",
        "                        # Empty pixels\n",
        "                        px += counts\n",
        "                    else:\n",
        "                        # Need to draw on these pixels, since we are drawing in vector form,\n",
        "                        # we need to draw horizontal lines on the image\n",
        "                        x_start = trunc(trunc(px / image_height) * adjusted_ratio)\n",
        "                        y_start = trunc(px % image_height * adjusted_ratio)\n",
        "                        px += counts\n",
        "                        x_end = trunc(trunc(px / image_height) * adjusted_ratio)\n",
        "                        y_end = trunc(px % image_height * adjusted_ratio)\n",
        "                        if x_end == x_start:\n",
        "                            # This is only on one line\n",
        "                            rle_list.append({'x': x_start, 'y': y_start, 'width': 1 , 'height': (y_end - y_start)})\n",
        "                        if x_end > x_start:\n",
        "                            # This spans more than one line\n",
        "                            # Insert top line first\n",
        "                            rle_list.append({'x': x_start, 'y': y_start, 'width': 1, 'height': (image_height - y_start)})\n",
        "                            \n",
        "                            # Insert middle lines if needed\n",
        "                            lines_spanned = x_end - x_start + 1 # total number of lines spanned\n",
        "                            full_lines_to_insert = lines_spanned - 2\n",
        "                            if full_lines_to_insert > 0:\n",
        "                                full_lines_to_insert = trunc(full_lines_to_insert * adjusted_ratio)\n",
        "                                rle_list.append({'x': (x_start + 1), 'y': 0, 'width': full_lines_to_insert, 'height': image_height})\n",
        "                                \n",
        "                            # Insert bottom line\n",
        "                            rle_list.append({'x': x_end, 'y': 0, 'width': 1, 'height': y_end})\n",
        "                if len(rle_list) > 0:\n",
        "                    rle_regions[segm['id']] = rle_list  \n",
        "            else:\n",
        "                # Add the polygon segmentation\n",
        "                for segmentation_points in segm['segmentation']:\n",
        "                    segmentation_points = np.multiply(segmentation_points, adjusted_ratio).astype(int)\n",
        "                    polygons_list.append(str(segmentation_points).lstrip('[').rstrip(']'))\n",
        "            polygons[segm['id']] = polygons_list\n",
        "            if i < len(self.colors):\n",
        "                poly_colors[segm['id']] = self.colors[i]\n",
        "            else:\n",
        "                poly_colors[segm['id']] = 'white'\n",
        "            \n",
        "            bbox = segm['bbox']\n",
        "            bbox_points = [bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1],\n",
        "                           bbox[0] + bbox[2], bbox[1] + bbox[3], bbox[0], bbox[1] + bbox[3],\n",
        "                           bbox[0], bbox[1]]\n",
        "            bbox_points = np.multiply(bbox_points, adjusted_ratio).astype(int)\n",
        "            bbox_polygons[segm['id']] = str(bbox_points).lstrip('[').rstrip(']')\n",
        "            \n",
        "            # Print details\n",
        "            print('    {}:{}:{}'.format(segm['id'], poly_colors[segm['id']], self.categories[segm['category_id']]))\n",
        "        \n",
        "        \n",
        "        \n",
        "        # Draw segmentation polygons on image\n",
        "        html  = '<div class=\"container\" style=\"position:relative;\">'\n",
        "        html += '<img src=\"{}\" style=\"position:relative;top:0px;left:0px;width:{}px;\">'.format(image_path, adjusted_width)\n",
        "        html += '<div class=\"svgclass\"><svg width=\"{}\" height=\"{}\">'.format(adjusted_width, adjusted_height)\n",
        "        \n",
        "        if show_polys:\n",
        "            for seg_id, points_list in polygons.items():\n",
        "                fill_color = poly_colors[seg_id]\n",
        "                stroke_color = poly_colors[seg_id]\n",
        "                for points in points_list:\n",
        "                    html += '<polygon points=\"{}\" style=\"fill:{}; stroke:{}; stroke-width:1; fill-opacity:0.5\" />'.format(points, fill_color, stroke_color)\n",
        "        \n",
        "        if show_crowds:\n",
        "            for seg_id, rect_list in rle_regions.items():\n",
        "                fill_color = poly_colors[seg_id]\n",
        "                stroke_color = poly_colors[seg_id]\n",
        "                for rect_def in rect_list:\n",
        "                    x, y = rect_def['x'], rect_def['y']\n",
        "                    w, h = rect_def['width'], rect_def['height']\n",
        "                    html += '<rect x=\"{}\" y=\"{}\" width=\"{}\" height=\"{}\" style=\"fill:{}; stroke:{}; stroke-width:1; fill-opacity:0.5; stroke-opacity:0.5\" />'.format(x, y, w, h, fill_color, stroke_color)\n",
        "            \n",
        "        if show_bbox:\n",
        "            for seg_id, points in bbox_polygons.items():\n",
        "                fill_color = poly_colors[seg_id]\n",
        "                stroke_color = poly_colors[seg_id]\n",
        "                html += '<polygon points=\"{}\" style=\"fill:{}; stroke:{}; stroke-width:1; fill-opacity:0\" />'.format(points, fill_color, stroke_color)\n",
        "                \n",
        "        html += '</svg></div>'\n",
        "        html += '</div>'\n",
        "        html += '<style>'\n",
        "        html += '.svgclass { position:absolute; top:0px; left:0px;}'\n",
        "        html += '</style>'\n",
        "        return html\n",
        "       \n",
        "    def process_info(self):\n",
        "        self.info = self.coco['info']\n",
        "    \n",
        "    def process_licenses(self):\n",
        "        self.licenses = self.coco['licenses']\n",
        "    \n",
        "    def process_categories(self):\n",
        "        self.categories = {}\n",
        "        self.super_categories = {}\n",
        "        for category in self.coco['categories']:\n",
        "            cat_id = category['id']\n",
        "            super_category = category['supercategory']\n",
        "            \n",
        "            # Add category to the categories dict\n",
        "            if cat_id not in self.categories:\n",
        "                self.categories[cat_id] = category\n",
        "            else:\n",
        "                print(\"ERROR: Skipping duplicate category id: {}\".format(category))\n",
        "\n",
        "            # Add category to super_categories dict\n",
        "            if super_category not in self.super_categories:\n",
        "                self.super_categories[super_category] = {cat_id} # Create a new set with the category id\n",
        "            else:\n",
        "                self.super_categories[super_category] |= {cat_id} # Add category id to the set\n",
        "                \n",
        "    def process_images(self):\n",
        "        self.images = {}\n",
        "        for image in self.coco['images']:\n",
        "            image_id = image['id']\n",
        "            if image_id in self.images:\n",
        "                print(\"ERROR: Skipping duplicate image id: {}\".format(image))\n",
        "            else:\n",
        "                self.images[image_id] = image\n",
        "                \n",
        "    def process_segmentations(self):\n",
        "        self.segmentations = {}\n",
        "        for segmentation in self.coco['annotations']:\n",
        "            image_id = segmentation['image_id']\n",
        "            if image_id not in self.segmentations:\n",
        "                self.segmentations[image_id] = []\n",
        "            self.segmentations[image_id].append(segmentation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzXCgArRB0kW"
      },
      "source": [
        "annotation_path = '../data/imgs/train/train.json'\n",
        "image_dir = '../data/imgs/train/'\n",
        "\n",
        "coco_dataset = CocoDataset(annotation_path, image_dir)\n",
        "coco_dataset.display_categories()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-q3xSRBB00Q"
      },
      "source": [
        "html = coco_dataset.display_image(1)\n",
        "IPython.display.HTML(html)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUeEdSjdKnbB"
      },
      "source": [
        "##**Setting Up Configurations**\n",
        "\n",
        "Look through the code cell below and update any lines relevant to your custom dataset.\n",
        "You may want to change:\n",
        "*  NAME (might want to be more specific)\n",
        "*  NUM_CLASSES (always 1 + the number of object categories you have)\n",
        "*  IMAGE_MIN_DIM (if you have larger training images)\n",
        "*  IMAGE_MAX_DIM (if you have larger training images)\n",
        "*  STEPS_PER_EPOCH (if you want to train on more images each epoch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK3WbbMfDS3k"
      },
      "source": [
        "class CustomConfig(Config):\n",
        "# Give the configuration a recognizable name    \n",
        "    NAME = \"skymaps\"\n",
        "\n",
        "    # Train on 1 GPU and 1 image per GPU. Batch size is 1 (GPUs * images/GPU).\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "    # Number of classes (including background)\n",
        "    NUM_CLASSES = 1 + 1  # Background + weed\n",
        "\n",
        "    # You can experiment with this number to see if it improves training\n",
        "    STEPS_PER_EPOCH = 100 #should be 1000\n",
        "\n",
        "    # This is how often validation is run. If you are using too much hard drive space\n",
        "    # on saved models (in the MODEL_DIR), try making this value larger.\n",
        "    VALIDATION_STEPS = 5\n",
        "\n",
        "    # Don't exclude based on confidence. Since we have two classes\n",
        "    # Skip detections with < 90% confidence\n",
        "    DETECTION_MIN_CONFIDENCE = 0.9\n",
        "\n",
        "    # Backbone network architecture\n",
        "    # Supported values are: resnet50, resnet101\n",
        "    BACKBONE = \"resnet50\"\n",
        "\n",
        "    # Input image resizing\n",
        "    # Random crops of size 512x512\n",
        "    IMAGE_RESIZE_MODE = \"crop\"\n",
        "    IMAGE_MIN_DIM = 512\n",
        "    IMAGE_MAX_DIM = 512\n",
        "    IMAGE_MIN_SCALE = 2.0\n",
        "\n",
        "    # Length of square anchor side in pixels\n",
        "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)\n",
        "\n",
        "    # ROIs kept after non-maximum supression (training and inference)\n",
        "    POST_NMS_ROIS_TRAINING = 1000\n",
        "    POST_NMS_ROIS_INFERENCE = 2000\n",
        "\n",
        "    # Non-max suppression threshold to filter RPN proposals.\n",
        "    # You can increase this during training to generate more propsals.\n",
        "    RPN_NMS_THRESHOLD = 0.9\n",
        "\n",
        "    # How many anchors per image to use for RPN training\n",
        "    RPN_TRAIN_ANCHORS_PER_IMAGE = 64\n",
        "\n",
        "    # Image mean (RGB)\n",
        "    MEAN_PIXEL = np.array([43.53, 39.56, 48.22])\n",
        "\n",
        "    # If enabled, resizes instance masks to a smaller size to reduce\n",
        "    # memory load. Recommended when using high-resolution images.\n",
        "    USE_MINI_MASK = True\n",
        "    MINI_MASK_SHAPE = (56, 56)  # (height, width) of the mini-mask\n",
        "\n",
        "    # Number of ROIs per image to feed to classifier/mask heads\n",
        "    # The Mask RCNN paper uses 512 but often the RPN doesn't generate\n",
        "    # enough positive proposals to fill this and keep a positive:negative\n",
        "    # ratio of 1:3. You can increase the number of proposals by adjusting\n",
        "    # the RPN NMS threshold.\n",
        "    TRAIN_ROIS_PER_IMAGE = 128\n",
        "\n",
        "    # Maximum number of ground truth instances to use in one image\n",
        "    MAX_GT_INSTANCES = 200\n",
        "\n",
        "    # Max number of final detections per image\n",
        "    DETECTION_MAX_INSTANCES = 400"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4M2RnocqzXI"
      },
      "source": [
        "# Code for Customdataset class. \n",
        "class CustomDataset(utils.Dataset):\n",
        "    \"\"\" Generates a COCO-like dataset, i.e. an image dataset annotated in the style of the COCO dataset.\n",
        "        See http://cocodataset.org/#home for more information.\n",
        "    \"\"\"\n",
        "    def load_custom(self, annotation_json, images_dir):\n",
        "        \"\"\" Load the coco-like dataset from json\n",
        "        Args:\n",
        "            annotation_json: The path to the coco annotations json file\n",
        "            images_dir: The directory holding the images referred to by the json file\n",
        "        \"\"\"\n",
        "        # Load json from file\n",
        "        json_file = open(annotation_json)\n",
        "        coco_json = json.load(json_file)\n",
        "        json_file.close()\n",
        "        \n",
        "        # Add the class names using the base method from utils.Dataset\n",
        "        source_name = \"skymaps\"\n",
        "        for category in coco_json['categories']:\n",
        "            class_id = category['id']\n",
        "            class_name = category['name']\n",
        "            #if class_id < 1:\n",
        "            #   print('Error: Class id for \"{}\" cannot be less than one. (0 is reserved for the background)'.format(class_name))\n",
        "            #   return\n",
        "            if class_id >= 1:\n",
        "              self.add_class(source_name, class_id, class_name)\n",
        "        \n",
        "        # Get all annotations\n",
        "        annotations = {}\n",
        "        for annotation in coco_json['annotations']:\n",
        "            image_id = annotation['image_id']\n",
        "            if image_id not in annotations:\n",
        "                annotations[image_id] = []\n",
        "            annotations[image_id].append(annotation)\n",
        "        \n",
        "        # Get all images and add them to the dataset\n",
        "        seen_images = {}\n",
        "        for image in coco_json['images']:\n",
        "            image_id = image['id']\n",
        "            if image_id in seen_images:\n",
        "                print(\"Warning: Skipping duplicate image id: {}\".format(image))\n",
        "            else:\n",
        "                seen_images[image_id] = image\n",
        "                try:\n",
        "                    image_file_name = image['file_name']\n",
        "                    image_width = image['width']\n",
        "                    image_height = image['height']\n",
        "                except KeyError as key:\n",
        "                    print(\"Warning: Skipping image (id: {}) with missing key: {}\".format(image_id, key))\n",
        "                \n",
        "                image_path = os.path.abspath(os.path.join(images_dir, image_file_name))\n",
        "                image_annotations = annotations[image_id]\n",
        "                \n",
        "                # Add the image using the base method from utils.Dataset\n",
        "                self.add_image(\n",
        "                    source=source_name,\n",
        "                    image_id=image_id,\n",
        "                    path=image_path,\n",
        "                    width=image_width,\n",
        "                    height=image_height,\n",
        "                    annotations=image_annotations\n",
        "                )\n",
        "                \n",
        "    def load_mask(self, image_id):\n",
        "        \"\"\" Load instance masks for the given image.\n",
        "        MaskRCNN expects masks in the form of a bitmap [height, width, instances].\n",
        "        Args:\n",
        "            image_id: The id of the image to load masks for\n",
        "        Returns:\n",
        "            masks: A bool array of shape [height, width, instance count] with\n",
        "                one mask per instance.\n",
        "            class_ids: a 1D array of class IDs of the instance masks.\n",
        "        \"\"\"\n",
        "        image_info = self.image_info[image_id]\n",
        "        annotations = image_info['annotations']\n",
        "        instance_masks = []\n",
        "        class_ids = []\n",
        "        \n",
        "        for annotation in annotations:\n",
        "            class_id = annotation['category_id']\n",
        "            mask = Image.new('1', (image_info['width'], image_info['height']))\n",
        "            mask_draw = ImageDraw.ImageDraw(mask, '1')\n",
        "            for segmentation in annotation['segmentation']:\n",
        "                mask_draw.polygon(segmentation, fill=1)\n",
        "                bool_array = np.array(mask) > 0\n",
        "                instance_masks.append(bool_array)\n",
        "                class_ids.append(class_id)\n",
        "                \n",
        "        mask = np.dstack(instance_masks)\n",
        "        class_ids = np.array(class_ids, dtype=np.int32)\n",
        "        \n",
        "        return mask, class_ids                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kkrb68mSm-Si"
      },
      "source": [
        "##**Create Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wBjni5yJ8fH"
      },
      "source": [
        "#LOAD MODEL. Create model in inference mode\n",
        "MODEL_DIR = \"/content/Mask_RCNN\"\n",
        "DEFAUTL_LOGS_DIR = os.path.join(MODEL_DIR, \"logs\")\n",
        "WEIGHTS_PATH = \"/content/Mask_RCNN/logs/skymaps20210727T2250/mask_rcnn_skymaps_0002.h5\"\n",
        "config = CustomConfig()\n",
        "config.display()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83BTW9XDmPVZ"
      },
      "source": [
        "# Create Model\n",
        "model = modellib.MaskRCNN(mode=\"training\", config=config,model_dir=DEFAUTL_LOGS_DIR)\n",
        "# Load COCO weights Or, load the last model you trained\n",
        "weights_path = WEIGHTS_PATH\n",
        "# Load weights\n",
        "print(\"Loading weights \", weights_path)\n",
        "# Exclude the last layers because they require a matching number of classes\n",
        "model.load_weights(weights_path, by_name=True, exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\"mrcnn_bbox\", \"mrcnn_mask\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ccUbgOqnIlJ"
      },
      "source": [
        "##**Start Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_Q8uF3NGo28"
      },
      "source": [
        "\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir /content/Mask_RCNN/logs/skymaps20210727T2250/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sg9HpwNqhEY"
      },
      "source": [
        "# Training dataset.\n",
        "dataset_train = CustomDataset()\n",
        "dataset_train.load_custom(\"/content/data/imgs/train/train.json\", \"/content/data/imgs/train/\")\n",
        "dataset_train.prepare()\n",
        "print(\"Images: {}\\nClasses: {}\".format(len(dataset_train.image_ids), dataset_train.class_names))\n",
        "\n",
        "# Validation dataset\n",
        "dataset_val = CustomDataset()\n",
        "dataset_val.load_custom(\"/content/data/imgs/validation/validation.json\", \"/content/data/imgs/validation/\")\n",
        "dataset_val.prepare()\n",
        "print(\"Images: {}\\nClasses: {}\".format(len(dataset_val.image_ids), dataset_val.class_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmcspOhnMHxt"
      },
      "source": [
        "!python -c 'import keras; print(keras.__version__)' #make sure 2.1.1 or mrcnn/model.py throws errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isp2nR-7tWJo"
      },
      "source": [
        "import time\n",
        "from PIL import Image, ImageDraw, ImageFont, ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "# Train the head branches\n",
        "# Passing layers=\"heads\" freezes all layers except the head\n",
        "# layers. You can also pass a regular expression to select\n",
        "# which layers to train by name pattern.\n",
        "print(\"Training network heads\")\n",
        "start_train = time.time()\n",
        "model.train(dataset_train, dataset_val,\n",
        "                learning_rate=config.LEARNING_RATE,\n",
        "                epochs=2,\n",
        "                layers='heads')\n",
        "end_train = time.time()\n",
        "minutes = round((end_train - start_train) / 60, 2)\n",
        "print(f'Training took {minutes} minutes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyJb8u-RbNh-"
      },
      "source": [
        "# Fine tune all layers\n",
        "# Passing layers=\"all\" trains all layers. You can also \n",
        "# pass a regular expression to select which layers to\n",
        "# train by name pattern.\n",
        "start_train = time.time()\n",
        "model.train(dataset_train, dataset_val, \n",
        "            learning_rate=config.LEARNING_RATE / 10,\n",
        "            epochs=2,layers=\"all\")\n",
        "end_train = time.time()\n",
        "minutes = round((end_train - start_train) / 60, 2)\n",
        "print(f'Training took {minutes} minutes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU9abGk3brV8"
      },
      "source": [
        "##**Detection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ2HOxqobqlk"
      },
      "source": [
        "class InferenceConfig(CustomConfig):\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "inference_config = InferenceConfig()\n",
        "\n",
        "# Recreate the model in inference mode\n",
        "model = modellib.MaskRCNN(mode=\"inference\", \n",
        "                          config=inference_config,\n",
        "                          model_dir=DEFAUTL_LOGS_DIR)\n",
        "\n",
        "# Get path to saved weights\n",
        "# Either set a specific path or find last trained weights\n",
        "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
        "model_path = model.find_last()\n",
        "\n",
        "# Load trained weights\n",
        "print(\"Loading weights from \", model_path)\n",
        "model.load_weights(model_path, by_name=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9ln85i3c-w2"
      },
      "source": [
        "# Test on a random image\n",
        "# Testing dataset.\n",
        "image_id = random.choice(dataset_val.image_ids)\n",
        "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "    modellib.load_image_gt(dataset_val, inference_config, \n",
        "                           image_id, use_mini_mask=False)\n",
        " \n",
        "log(\"original_image\", original_image)\n",
        "log(\"image_meta\", image_meta)\n",
        "log(\"gt_class_id\", gt_class_id)\n",
        "log(\"gt_bbox\", gt_bbox)\n",
        "log(\"gt_mask\", gt_mask)\n",
        " \n",
        "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
        "                            dataset_train.class_names, figsize=(8, 8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SV9QaKCztXJC"
      },
      "source": [
        "## **Evaluate Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IASKeUMgsna"
      },
      "source": [
        "# Testing dataset.\n",
        "dataset_test = CustomDataset()\n",
        "dataset_test.load_custom(\"/content/data/imgs/test/test.json\", \"/content/data/imgs/test/\")\n",
        "dataset_test.prepare()\n",
        "print(\"Images: {}\\nClasses: {}\".format(len(dataset_test.image_ids), dataset_test.class_names))\n",
        " \n",
        "# Compute VOC-Style mAP @ IoU=0.5\n",
        "# Running on 10 images. Increase for better accuracy.\n",
        "image_ids = np.random.choice(dataset_test.image_ids, 234)\n",
        "APs = []\n",
        "for image_id in image_ids:\n",
        "    # Load image and ground truth data\n",
        "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "        modellib.load_image_gt(dataset_test, inference_config,\n",
        "                               image_id, use_mini_mask=False)\n",
        "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
        "    # Run object detection\n",
        "    results = model.detect([image], verbose=0)\n",
        "    r = results[0]\n",
        "    # Compute AP\n",
        "    AP, precisions, recalls, overlaps =\\\n",
        "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
        "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
        "    APs.append(AP)\n",
        "    \n",
        "print(\"mAP: \", np.mean(APs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-iuawwfNtrz"
      },
      "source": [
        "while True:pass"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}